# Importing library
import pickle  # Reading processed data from the file
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation
from keras.utils.vis_utils import plot_model
import pydot  # Used to plot model architecture
import seaborn as sns  # Used to show the counts of observations in each category.
import matplotlib.pyplot as plt  # Used to plot graphs.
from sklearn.metrics import confusion_matrix  # Used to plot confusion matrix.


# Loading preprocessed data from the file
pickle_in = open('./Dataset/X.pickle', 'rb')
X = pickle.load(pickle_in)  # It has Features
pickle_in = open('./Dataset/Y.pickle', 'rb')
Y = pickle.load(pickle_in)  # It has Labels


# Normalizing the data in the range of [0, 1]
X = X / 255.0
print(X)


# Building the model
model = Sequential()
model.add (Conv2D (256, (3, 3), input_shape = (100,100,3)))  # Number of Neurons = 256, filter size = 3*3, Dimension of color image = (100,100,3).
model.add (Activation ('relu'))  # Convert negative value zero but do not change positive values.
model.add (MaxPooling2D (pool_size = (2, 2)))  # Pool_size is the size of filter.
model.add (Conv2D (256, (3, 3)))
model.add (Activation ('relu'))
model.add (MaxPooling2D (pool_size = (2, 2)))
model.add (Conv2D (256, (3, 3)))
model.add (Activation ('relu'))
model.add (MaxPooling2D (pool_size = (2, 2)))
model.add (Flatten())  # Convert 3D feature map to 1D feature vector.
model.add (Dense(64))  # Also known as "Fully Connected layer", it connent each input to each output. it has 64 neurons.
model.add (Dropout(0.3))
model.add (Dense(1))  # It has only 1 neurons.
model.add (Activation('sigmoid'))  # Sigmoid is a binary function.


# Printing summary of the model
print(model.summary())


# Compiling the model
model.compile (loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.optimizer.lr = 0.0001


# Training the model & storing its detail in the variable r
h = model.fit (X, Y, batch_size = 32, epochs = 50, validation_split = 0.3)


# Saving the model in hierarchical Data Formate (HDF)
model.save('./My_Model/My_Model_50.h5')


# Ploting Architecture of the model
plot_model(model, to_file='./My_Model/My_Model_Layers.png')


# Plot Accuracy & Loss Graphs
plt.plot(h.history["accuracy"])
plt.plot(h.history['val_accuracy'])
plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title("Loss & Accuracy Model")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(["Accuracy","Validation Accuracy","Loss","Validation Loss"])
plt.show()
plt.savefig("./My_Model/My_Model_Loss_Accuracy_Graph_50.png")


# Comparing probability with the real labels
plt.plot(model.predict(X),'.',color='red',label='Predicted Probabilty')
plt.plot(Y,'.',color='navy',label='Actual Labels')
plt.xlabel('Total Number of Data')
plt.ylabel('Probability')
plt.legend()
plt.savefig('./My_Model/My_Model_Actual_vs_Predicted_Result_50.png')


# Printing Confusion matrix
sns.heatmap(confusion_matrix(decision,Y),cmap='plasma',annot=True,annot_kws={"size": 25})
plt.title('Confusion matrix ',fontsize=20)
plt.xlabel("Predicted labels",fontsize=15)
plt.ylabel("True labels",fontsize=15)
plt.xticks([0.50,1.50],["With_Mask", "Without_Mask"])
plt.yticks([0.5,1.5],["With_Mask", "Without_Mask"])
plt.show()
plt.savefig('./My_Model/My_Model_Heatmap_With_Numbres.png')


# Printing Confusion matrix
cm = metrics.confusion_matrix(Y, decision)
plt.imshow(cm, cmap=plt.cm.Blues)
plt.title('Confusion matrix ',fontsize=20)
plt.xlabel("Predicted labels",fontsize=15)
plt.ylabel("True labels",fontsize=15)
plt.xticks([0.05,1.05],["With_Mask", "Without_Mask"])
plt.yticks([0.025,1.05],["With_Mask", "Without_Mask"])
plt.show()
plt.savefig('./My_Model/My_Model_Heatmap_Without_Numbres.png')
